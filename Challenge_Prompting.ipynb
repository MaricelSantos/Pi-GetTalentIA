{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92de8ca4-7181-45f5-9980-b723d2566f5e",
   "metadata": {},
   "source": [
    "# Challenge Prompting\n",
    "\n",
    "Resolver los siguientes ejercicios dejando el codigo con su ejecucion.\n",
    "\n",
    "Importar las librerias necesarias y **correr las celdas para visualizar el resultado en cada ejercicio**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d65bbb09-0f5a-4e97-9a06-2361c5cdd1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## bloque importacion de librerias\n",
    "\n",
    "import json\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e004e5b7-b704-4592-b8b4-b01b8d6687cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOepoIgRKX8xB0mGePFoatDW0vSWgcFLVxa9xbhq\n"
     ]
    }
   ],
   "source": [
    "## bloque variables de entorno\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()  # Load .env file\n",
    "\n",
    "api_key = os.getenv(\"COHERE_API_KEY\")\n",
    "print(api_key)  # Verify the key is loaded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd6745ff-cd45-4231-b3d6-518954a9ffca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id='c03b7f53-2e14-4923-ab2d-2282d9e52951' finish_reason='COMPLETE' prompt=None message=AssistantMessageResponse(role='assistant', tool_calls=None, tool_plan=None, content=[TextAssistantMessageResponseContentItem(type='text', text='Soy Command R+ 08-2024, un modelo de lenguaje de gran tamaño desarrollado por la empresa Cohere. Estoy diseñado para ayudar a los usuarios humanos proporcionando respuestas detalladas y sofisticadas. Mi objetivo es ofrecer información útil, responder preguntas y ayudar en diversas tareas a través de conversaciones naturales y fluidas.\\n\\nSi deseas saber más sobre mis capacidades o tienes alguna otra pregunta, no dudes en preguntar. Estoy aquí para ayudarte.')], citations=None) usage=Usage(billed_units=UsageBilledUnits(input_tokens=4.0, output_tokens=98.0, search_units=None, classifications=None), tokens=UsageTokens(input_tokens=205.0, output_tokens=98.0)) logprobs=None\n",
      "Soy Command R+ 08-2024, un modelo de lenguaje de gran tamaño desarrollado por la empresa Cohere. Estoy diseñado para ayudar a los usuarios humanos proporcionando respuestas detalladas y sofisticadas. Mi objetivo es ofrecer información útil, responder preguntas y ayudar en diversas tareas a través de conversaciones naturales y fluidas.\n",
      "\n",
      "Si deseas saber más sobre mis capacidades o tienes alguna otra pregunta, no dudes en preguntar. Estoy aquí para ayudarte.\n"
     ]
    }
   ],
   "source": [
    "## bloque conexion a Cohere\n",
    "\n",
    "import cohere\n",
    "co = cohere.ClientV2(log_warning_experimental_features=False)\n",
    "# alternativa:\n",
    "# co = cohere.ClientV2(api_key)\n",
    "\n",
    "response = co.chat(\n",
    "    model=\"command-r-plus-08-2024\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"quién sos?\"}],\n",
    ")\n",
    "\n",
    "print(response)\n",
    "print(response.message.content[0].text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aacdb26-ce51-49cc-b37f-5aa45c09ff51",
   "metadata": {},
   "source": [
    "## Ejercicio 1\n",
    "\n",
    "Extraccion de entidades\n",
    "\n",
    "Utilizar el LLM para extraer las siguientes entidades del texto medico.\n",
    "\n",
    "- Paciente:\n",
    "    - Nombre\n",
    "    - Edad\n",
    "- Fecha de admisión\n",
    "- Síntomas\n",
    "- Diagnóstico\n",
    "- Tratamiento recomendado\n",
    "\n",
    "**Aclaracion:** \n",
    "\n",
    "La salida tiene que ser un **string con formato de tipo json**, el cual se convertira en un diccionario de Python.\n",
    "\n",
    "Si la linea de conversion en test da error el ejercicio no esta completo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750e43d7-b074-4973-9cd0-5a6fbe816084",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ejemplo \n",
    "\n",
    "# texto a analizar\n",
    "\"\"\"La paciente, María González, de 45 años, fue admitida en el Hospital Central el 5 de agosto de 2023 debido a síntomas de fatiga crónica y dolores musculares./\n",
    "Tras una serie de análisis, se diagnosticó fibromialgia. La doctora a cargo, Laura Ramírez, recomendó un tratamiento basado en fisioterapia y medicamentos analgésicos. /\n",
    "La próxima consulta está programada para el 15 de septiembre.\"\"\"\n",
    "\n",
    "\n",
    "# respuesta del LLM\n",
    "{\n",
    "  \"paciente\": {\n",
    "    \"nombre\": \"María González\",\n",
    "    \"edad\": 45\n",
    "  },\n",
    "  \"fecha_admision\": \"2023-08-05\",\n",
    "  \"sintomas\": [\n",
    "    \"fatiga crónica\",\n",
    "    \"dolores musculares\"\n",
    "  ],\n",
    "  \"diagnostico\": \"fibromialgia\",\n",
    "  \"tratamiento\": [\n",
    "    \"fisioterapia\",\n",
    "    \"medicamentos analgésicos\"\n",
    "  ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38d9cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Solución propuesta\n",
    "\n",
    "def datos_pacientes(consulta):\n",
    "    \"\"\"\n",
    "    Leer datos de pacientes en historias clinicas y extraer datos.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"Extrae datos del paciente en formato JSON\n",
    "    \n",
    "             texto: ''' {consulta} ''' \n",
    "\n",
    "            Datos paciente:\n",
    "            - Paciente:\n",
    "                - Nombre\n",
    "                - Edad\n",
    "            - Fecha de admisión\n",
    "            - Síntomas (lista)\n",
    "            - Diagnóstico\n",
    "            - Tratamiento recomendado (lista)\n",
    "    \"\"\"\n",
    "    response = co.chat(\n",
    "        model=\"command-r-plus-08-2024\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\":\"Extrae datos de pacientes a partir de la historia clinica\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        response_format={\"type\": \"json_object\"}\n",
    "    )\n",
    "    \n",
    "    return response.message.content[0].text\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "04ef7446",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_to_analize = \"\"\"La paciente, María González, de 45 años, fue admitida en el Hospital Central el 5 de agosto de 2023 debido a síntomas de fatiga crónica y dolores musculares./\n",
    "Tras una serie de análisis, se diagnosticó fibromialgia. La doctora a cargo, Laura Ramírez, recomendó un tratamiento basado en fisioterapia y medicamentos analgésicos. /\n",
    "La próxima consulta está programada para el 15 de septiembre.\"\"\"\n",
    "\n",
    "caso_1 = \"Cardiopatía isquémica - La paciente, Rosa Martínez, de 62 años, ingresó al Hospital San José el 12 de junio de 2024 por presentar dolor torácico persistente y dificultad para respirar. / Los exámenes de electrocardiograma y análisis de enzimas cardíacas confirmaron un infarto agudo de miocardio. / El doctor Pedro García realizó una angioplastia de emergencia con éxito, y la paciente fue derivada a rehabilitación cardíaca. / Su control postoperatorio está programado para el 20 de junio.\"\n",
    "caso_2 = \"Diabetes mellitus tipo 2 - El paciente, Juan López, de 54 años, acudió al Centro Médico de la Ciudad el 15 de mayo de 2023 debido a fatiga constante, sed excesiva y pérdida de peso involuntaria. / Los análisis de sangre revelaron niveles de glucosa de 280 mg/dL en ayunas y hemoglobina glicosilada elevada, diagnosticándose diabetes tipo 2. / La endocrinóloga Clara Fernández inició tratamiento con metformina y cambios en la dieta. / Su próxima consulta de seguimiento será el 10 de junio.\"\n",
    "\n",
    "caso_3 = \"Apendicitis aguda - El paciente, Lucas Rodríguez, de 28 años, se presentó en la guardia del Hospital General el 8 de marzo de 2024 con dolor abdominal agudo en la fosa iliaca derecha, fiebre y náuseas. / Una ecografía abdominal confirmó apendicitis aguda. / Fue sometido a una apendicectomía laparoscópica realizada por el cirujano Carlos Méndez. / El alta médica se otorgó el 10 de marzo con indicaciones de reposo y control de signos de infección. / El seguimiento está programado para el 18 de marzo.\"\n",
    "\n",
    "caso_4 = \"Asma bronquial - María Fernández, de 32 años, consultó en la Clínica Respiratoria el 25 de abril de 2023 por episodios recurrentes de sibilancias, tos y dificultad para respirar, especialmente durante la noche. / Las pruebas de función pulmonar confirmaron un diagnóstico de asma moderada persistente. / La neumóloga Ana López recetó un inhalador con corticosteroides y broncodilatadores de acción rápida para uso según necesidad. / Un control de seguimiento se agendó para el 25 de mayo.\"\n",
    "\n",
    "caso_5 = text_to_analize\n",
    "\n",
    "\n",
    "casos = [globals()[f\"caso_{i}\"] for i in range(1, 6)]\n",
    "\n",
    "\n",
    "resultados = [datos_pacientes(caso) for caso in casos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c36e919d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"paciente\": {\n",
      "        \"nombre\": \"Juan López\",\n",
      "        \"edad\": 54\n",
      "    },\n",
      "    \"fecha_admision\": \"15 de mayo de 2023\",\n",
      "    \"sintomas\": [\n",
      "        \"Fatiga constante\",\n",
      "        \"Sed excesiva\",\n",
      "        \"Pérdida de peso involuntaria\"\n",
      "    ],\n",
      "    \"diagnostico\": \"Diabetes mellitus tipo 2\",\n",
      "    \"tratamiento\": [\n",
      "        \"Metformina\",\n",
      "        \"Cambios en la dieta\"\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(resultados[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6f0009-8a9c-49a3-b740-d73e0dbe765e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'paciente': {'nombre': 'Juan López', 'edad': 54}, 'fecha_admision': '15 de mayo de 2023', 'sintomas': ['Fatiga constante', 'Sed excesiva', 'Pérdida de peso involuntaria'], 'diagnostico': 'Diabetes mellitus tipo 2', 'tratamiento': ['Metformina', 'Cambios en la dieta']}\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "final_result1 = json.loads(resultados[1])\n",
    "print(final_result1)\n",
    "# test todos los ejemplso\n",
    "final_result = [json.loads(resultado) for resultado in resultados]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37fbf25-6db4-432a-82c4-2e7edce27686",
   "metadata": {},
   "source": [
    "## Ejercicio 2\n",
    "\n",
    "Tenemos dos funciones en Python, una llamada *'add_contact'* y otra llamada *'get_information'*.\n",
    "\n",
    "**Utilizar algun LLM que permita funtion calling** y desarrollar un codigo secuencial automatico que consiga:\n",
    "\n",
    "Interpretar la consulta del usuario, identificar a que funcion llamar, luego llamarla (si es que aplica) y darle una respuesta final al usuario.  (usar function calling para esta solucion)\n",
    "\n",
    "La entrada a dicho codigo es la consulta del usuario, a continuacion algunos ejemplos:\n",
    "\n",
    "- \"Agrega a Juan Pérez con el número 555-1234 y el correo juanperez@mail.com.\"\n",
    "- \"Guarda a Lucía Gómez en mis contactos. Su teléfono es 555-5678 y su email es lucia.gomez@gmail.com.\"\n",
    "- \"Cual es el Email de Juan Pérez.?\"\n",
    "\n",
    "Salidas esperadas de dichos ejemplos (variaran porque las genera el LLM):\n",
    "-  El contacto fue anadido con exito\n",
    "-  Se anadio el contacto\n",
    "-  El email de juan perez es juanperez@mail.com\n",
    "\n",
    "Link de ayuda: https://github.com/cohere-ai/notebooks/blob/main/notebooks/agents/Vanilla_Tool_Use_v2.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "3632856c",
   "metadata": {},
   "outputs": [],
   "source": [
    "contacts = {\n",
    "                        'Joaquin Lopez':{'tel': 15456663258, 'mail': 'Joacolocolopez@gmail.com'},\n",
    "                      'Flavio Oncativo':{'tel': 1545554178, 'mail': 'FOncativo@hotmail.com'}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "c6e43cb9-5e6a-4807-9818-c85408f1ba58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_contact(name, phone, email):\n",
    "    \"\"\"\n",
    "    Agrega un contacto al diccionario.\n",
    "    Parámetros:\n",
    "        name (str): Nombre del contacto.\n",
    "        phone (str): Número de teléfono del contacto.\n",
    "        email (str): Correo electrónico del contacto.\n",
    "    Retorna:\n",
    "        str: Mensaje confirmando la adición del contacto.\n",
    "    \"\"\"\n",
    "    contacts[name] = {'phone': phone, 'email': email}\n",
    "    return \"Contacto anadido con exito.\"\n",
    "\n",
    "def get_information(name):\n",
    "    \"\"\"\n",
    "    Recupera la información de un contacto.\n",
    "    Parámetros:\n",
    "        name (str): Nombre del contacto.\n",
    "    Retorna:\n",
    "        dict/str: Información del contacto o un mensaje si no existe.\n",
    "    \"\"\"\n",
    "    if name in contacts:\n",
    "        return contacts[name]\n",
    "    else:\n",
    "        return \"Contacto no encontrado.\"\n",
    "    \n",
    "functions_map = {\n",
    "    \"add_contact\": add_contact,\n",
    "    \"get_information\": get_information\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "09bf5950",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"add_contact\",\n",
    "            \"description\": \"Agrega un contacto al diccionario.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"name\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"Nombre del contacto.\"\n",
    "                    },\n",
    "                    \"phone\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"Número de teléfono del contacto.\"\n",
    "                    },\n",
    "                    \"email\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"Correo electrónico del contacto.\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"name\", \"phone\", \"email\"]\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_information\",\n",
    "            \"description\": \"Recupera la información de un contacto.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"name\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"Nombre del contacto.\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"name\"]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "c58b6496",
   "metadata": {},
   "outputs": [],
   "source": [
    "preamble = \"\"\"\n",
    "## Task and Context\n",
    "You help people to add contacts, emails and phones.You have instructions ans tools to do that. \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# user request\n",
    "request = \"Agrega a Juan Pérez con el número 555-1234 y el correo juanperez@mail.com.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "b8cf1239",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model recommends doing the following tool calls:\n",
      "\n",
      "Tool plan:\n",
      "I will add the contact using the add_contact tool. \n",
      "\n",
      "Tool calls:\n",
      "Tool name: add_contact | Parameters: {\"email\":\"juanperez@mail.com\",\"name\":\"Juan Pérez\",\"phone\":\"555-1234\"}\n"
     ]
    }
   ],
   "source": [
    "messages=[{\"role\": \"system\", \"content\": preamble},\n",
    "          {\"role\": \"user\", \"content\": request}]\n",
    "\n",
    "response = co.chat(\n",
    "    model=\"command-r-plus\",\n",
    "    messages=messages,\n",
    "    tools=tools\n",
    ")\n",
    "\n",
    "print(\"The model recommends doing the following tool calls:\\n\")\n",
    "print(\"Tool plan:\")\n",
    "print(response.message.tool_plan,\"\\n\")\n",
    "print(\"Tool calls:\")\n",
    "for tc in response.message.tool_calls:\n",
    "    print(f\"Tool name: {tc.function.name} | Parameters: {tc.function.arguments}\")\n",
    "    \n",
    "# append the chat history\n",
    "messages.append({'role': 'assistant', 'tool_calls': response.message.tool_calls, 'tool_plan': response.message.tool_plan})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "a5f82faa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool results that will be fed back to the model in step 4:\n",
      "\"Contacto anadido con exito.\"\n"
     ]
    }
   ],
   "source": [
    "tool_content = []\n",
    "# Iterate over the tool calls generated by the model\n",
    "for tc in response.message.tool_calls:\n",
    "    # here is where you would call the tool recommended by the model, using the parameters recommended by the model\n",
    "    tool_result= functions_map[tc.function.name](**json.loads(tc.function.arguments))\n",
    "    # store the output in a list\n",
    "    tool_content.append(json.dumps(tool_result))\n",
    "    # append the chat history\n",
    "    messages.append({'role': 'tool', 'tool_call_id': tc.id, 'content': tool_content}) \n",
    "\n",
    "print(\"Tool results that will be fed back to the model in step 4:\")\n",
    "for result in tool_content:\n",
    "    print(json.dumps(json.loads(result), indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "8a09230a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final answer:\n",
      "Contacto añadido con éxito.\n"
     ]
    }
   ],
   "source": [
    "response = co.chat(\n",
    "    model=\"command-r-plus\",\n",
    "    messages=messages,\n",
    "    tools=tools\n",
    ")\n",
    "print(\"Final answer:\")\n",
    "print(response.message.content[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36ef67d-dd54-41e7-bef5-f89ee575603f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TIPS\n",
    "# Probar primero generando una funcion y llamarla, luego anadir la otra\n",
    "# Plantearlo paso por paso en distintas celdas, analizar las salidas y las entradas, como identificamos a que funcion llamar?\n",
    "# luego automatizar dentro de una sola celda\n",
    "\n",
    "\n",
    "# Lo importante es entregar hasta donde lleguen, sea una funcion, las dos pero sin poder hacer el flujo automatico, lo que puedan, siempre y cuando este\n",
    "# claro lo que se quizo hacer con comentarios."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d8badf-dc90-4005-b916-8e528105d797",
   "metadata": {},
   "source": [
    "## Ejercicio 3\n",
    "\n",
    "Crear una funcion llamada \"history_answer\", que toma como parametro de entrada una pregunta sobre un contexto dado y la salida es la respuesta final del proceso impulsado por un LLM.\n",
    "\n",
    "Dada una historia, el usuario podra hacer preguntas sobre la misma y el LLM debe responder siguiendo los siguientes lineamientos:\n",
    "\n",
    "REQUISITOS DE LA RESPUESTA\n",
    "- las respuestas deben ser en base a la historia\n",
    "- ante la misma pregunta siempre debe responder de la misma manera.\n",
    "- que responda en solo una oracion.\n",
    "- el idioma que responde debe ser el mismo que con el que se pregunta (ingles, espanol, portugues).\n",
    "- que agregue emojis en la oracion que resuman el contenido de la misma.\n",
    "- que responda siempre en tercera persona.\n",
    "- si la pregunta no tiene relacion alguna con el contexto, la respuesta debe ser 'Lo siento no puedo ayudarte con eso'.\n",
    "- Responder con 'Hakuna Matata!' al final de **todas** las respuestas (no importa idioma ni cantidad de tokens).\n",
    "\n",
    "**Ayudin**: \n",
    "- No se limiten a usar 1 solo request al LLM, pueden dividirlo en partes para que por un lado se verifique el idioma, por otro lado se verifique si la pregunta tiene relacion con el contexto, etc\n",
    "\n",
    "- Estructuren bien el prompt procurando separar instrucciones, contexto(historia) y pregunta del usuario.\n",
    "\n",
    "- Recuerden usar el system message y user message.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f36820-b7d3-4813-a0c3-351c598106ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ejemplo flojo de estructura de prompt\n",
    "# prompt = f\"Responde a la pregunta: {pregunta} de manera concisa y divertida en base a la siguiente historia: {historia}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c5602f56-4032-4cc6-a2c5-7b29cf2a2a5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La historia tuvo lugar en un feudo medieval, donde la guerra y la batalla marcaron el destino de Thomas. 🏰🗡️ Hakuna Matata!\n",
      "History took place in a medieval fiefdom, where war and battle shaped Thomas's destiny. 🏰🗡️ Hakuna Matata!\n"
     ]
    }
   ],
   "source": [
    "# Primera iteracion\n",
    "historia = \"\"\"En un pequeño feudo medieval, Thomas, un joven campesino de dieciséis años, trabajaba desde el amanecer en los campos de trigo del señor feudal. El sol apenas había salido cuando él ya había arado más de lo que sus manos podían soportar. La vida era dura, pero su familia dependía de la cosecha para pagar los impuestos y mantener su hogar de madera y paja.\n",
    "\n",
    "Un día, el feudo fue sacudido por noticias de guerra. El rey había llamado a todos los hombres en edad de luchar. Thomas sabía que, al igual que otros jóvenes, no tenía elección. Cambió la hoz por una lanza rudimentaria y se unió a la milicia local. Sin entrenamiento, fue empujado a un campo de batalla embarrado, donde el acero resonaba y los gritos de los hombres llenaban el aire.\n",
    "\n",
    "La batalla fue un caos. Thomas, con el corazón latiendo en su pecho como un tambor de guerra, apenas podía distinguir amigo de enemigo. Logró esquivar una espada, pero cayó al suelo, cubierto de lodo y sangre. Levantándose, vio cómo un compañero caía junto a él, sus ojos abiertos, vacíos.\n",
    "\n",
    "Cuando la batalla terminó, el silencio era tan profundo como el vacío que sentía. Thomas regresó al feudo, diferente, marcado por la muerte y la violencia. Su madre lo recibió con lágrimas en los ojos, pero él, con la mirada fija en el horizonte, sabía que la inocencia había quedado atrás, enterrada en aquel campo de batalla. La paz del feudo ya no era la misma; él tampoco.\"\"\"\n",
    "\n",
    "\n",
    "pregunta = \"Where did history happened?\"\n",
    "\n",
    "system_prompt = \"Tu tarea es responder las preguntas, en el mismo idioma en que fueron realizada, utilizando el contexto como base de informacion\"\n",
    "\n",
    "prompt = f\"\"\" \n",
    "            ###\n",
    "            Instrucciones: \n",
    "            - Responde la pregunta utilizado el contexto.\n",
    "            - Responde de manera concisa, en una sola oracion.\n",
    "            - Agrega emojis relacionados en la respuesta.\n",
    "            - Responde en el mismo idioma\n",
    "            - Si la pregunta no se responde con el contexto, la respuesta debe ser 'Lo siento no puedo ayudarte con eso'.\n",
    "            - Responder con 'Hakuna Matata!' al final de **todas** las respuestas (no importa idioma ni cantidad de tokens).\n",
    "            - Responde en tercera persona\n",
    "\n",
    "            ###\n",
    "            Contexto:\n",
    "            {historia}\n",
    "\n",
    "            ###\n",
    "            Pregunta:\n",
    "            {pregunta}\n",
    "\n",
    "\n",
    "\n",
    "            \"\"\"\n",
    "\n",
    "response_sintraduccion= co.chat(\n",
    "    model=\"command-r-plus-08-2024\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\":system_prompt},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ],\n",
    "    seed = 28\n",
    ")\n",
    "\n",
    "prompt_traductor = f\"\"\"\n",
    "                    Identifica el idioma en '''{pregunta}'''. \n",
    "                    Traduce '''{response_sintraduccion}''' exactamente al idioma identificado\n",
    "                    No agregar texto\n",
    "                    Responde sin explicación ni contexto\n",
    "\"\"\"\n",
    "prompt_traductor = f\"\"\"\n",
    "                    Usa el idioma de '''{pregunta}'''. \n",
    "                    para traducir '''{response_sintraduccion}'''\n",
    "\"\"\"\n",
    "response = co.chat(\n",
    "        model=\"command-r-plus-08-2024\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\":\"Traducir respuesta al idioma de la pregunta\"},\n",
    "            {\"role\": \"user\", \"content\": prompt_traductor}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "# imprimir la respuesta\n",
    "print(response_sintraduccion.message.content[0].text)\n",
    "print(response.message.content[0].text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4fa65537-3aa6-43c6-87e9-86a689f8e1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def history_answer(pregunta):\n",
    "    system_prompt = \"Tu tarea es responder las preguntas, en el mismo idioma en que fueron realizada, utilizando el contexto como base de informacion\"\n",
    "\n",
    "    prompt = f\"\"\" \n",
    "            ###\n",
    "            Instrucciones: \n",
    "            - Responde la pregunta utilizado el contexto.\n",
    "            - Responde de manera concisa, en una sola oracion.\n",
    "            - Agrega emojis relacionados en la respuesta.\n",
    "            - Responde en el mismo idioma\n",
    "            - Si la pregunta no se responde con el contexto, la respuesta debe ser 'Lo siento no puedo ayudarte con eso'.\n",
    "            - Responder con 'Hakuna Matata!' al final de **todas** las respuestas (no importa idioma ni cantidad de tokens).\n",
    "            - Responde en tercera persona\n",
    "\n",
    "            ###\n",
    "            Contexto:\n",
    "            {historia}\n",
    "\n",
    "            ###\n",
    "            Pregunta:\n",
    "            {pregunta}\n",
    "\n",
    "\n",
    "\n",
    "            \"\"\"\n",
    "\n",
    "    response_sintraduccion= co.chat(\n",
    "            model=\"command-r-plus-08-2024\",\n",
    "            messages=[\n",
    "                    {\"role\": \"system\", \"content\":system_prompt},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            seed = 28\n",
    "    )\n",
    "\n",
    "    prompt_traductor = f\"\"\"\n",
    "                    Identifica el idioma en '''{pregunta}'''. \n",
    "                    Traduce '''{response_sintraduccion}''' exactamente al idioma identificado\n",
    "                    No agregar texto\n",
    "                    Responde sin explicación ni contexto\n",
    "    \"\"\"\n",
    "    prompt_traductor = f\"\"\"\n",
    "                    Usa el idioma de '''{pregunta}'''. \n",
    "                    para traducir '''{response_sintraduccion}'''\n",
    "    \"\"\"\n",
    "    response = co.chat(\n",
    "            model=\"command-r-plus-08-2024\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\":\"Traducir respuesta al idioma de la pregunta\"},\n",
    "                {\"role\": \"user\", \"content\": prompt_traductor}\n",
    "           ]\n",
    "    )\n",
    "\n",
    "\n",
    "    return response.message.content[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f3aaa967-d87f-45d9-9a72-6fc63afc931c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thomas es un valiente joven campesino que se convirtió en soldado, enfrentando la guerra y la pérdida de su inocencia. ¡Hakuna Matata!\n",
      "Thomas is a young peasant who became a soldier in a medieval war. 🗡️🛡️ Hakuna Matata!\n",
      "Lo siento, no puedo ayudarte con esa traducción. ¡Hakuna Matata! 🌾🗡️🛡️\n",
      "¡Hola! Soy Thomas, un joven campesino que se transformó en soldado. ¡Hakuna Matata!\n",
      "El rey convocó a todos los hombres para la batalla. 🗡️🛡️ ¡Hakuna Matata!\n"
     ]
    }
   ],
   "source": [
    "historia = \"\"\"En un pequeño feudo medieval, Thomas, un joven campesino de dieciséis años, trabajaba desde el amanecer en los campos de trigo del señor feudal. El sol apenas había salido cuando él ya había arado más de lo que sus manos podían soportar. La vida era dura, pero su familia dependía de la cosecha para pagar los impuestos y mantener su hogar de madera y paja.\n",
    "\n",
    "Un día, el feudo fue sacudido por noticias de guerra. El rey había llamado a todos los hombres en edad de luchar. Thomas sabía que, al igual que otros jóvenes, no tenía elección. Cambió la hoz por una lanza rudimentaria y se unió a la milicia local. Sin entrenamiento, fue empujado a un campo de batalla embarrado, donde el acero resonaba y los gritos de los hombres llenaban el aire.\n",
    "\n",
    "La batalla fue un caos. Thomas, con el corazón latiendo en su pecho como un tambor de guerra, apenas podía distinguir amigo de enemigo. Logró esquivar una espada, pero cayó al suelo, cubierto de lodo y sangre. Levantándose, vio cómo un compañero caía junto a él, sus ojos abiertos, vacíos.\n",
    "\n",
    "Cuando la batalla terminó, el silencio era tan profundo como el vacío que sentía. Thomas regresó al feudo, diferente, marcado por la muerte y la violencia. Su madre lo recibió con lágrimas en los ojos, pero él, con la mirada fija en el horizonte, sabía que la inocencia había quedado atrás, enterrada en aquel campo de batalla. La paz del feudo ya no era la misma; él tampoco.\"\"\"\n",
    "\n",
    "\n",
    "pregunta1 = \"Quién es Thomas?\"\n",
    "pregunta2 = \"Who is Thomas?\"\n",
    "pregunta3 = \"Quién es Lilia?\"\n",
    "pregunta4 = \"Quién sos?\" #Chequeo de tercera persona\n",
    "pregunta5 = \"Qué hizo el rey?\"\n",
    "\n",
    "\n",
    "# respuesta\n",
    "print(history_answer(pregunta1))\n",
    "\n",
    "# respuesta\n",
    "print(history_answer(pregunta2))\n",
    "\n",
    "# respuesta\n",
    "print(history_answer(pregunta3))\n",
    "\n",
    "# respuesta\n",
    "print(history_answer(pregunta4))\n",
    "\n",
    "# respuesta\n",
    "print(history_answer(pregunta5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6e9a7d-aa39-4a01-ba55-9a7f4ea39522",
   "metadata": {},
   "source": [
    "## Ejercicio 4\n",
    "\n",
    "Crear un chatbot sencillo impulsado por un LLM. \n",
    "\n",
    "Dicho bot esta destinado a un usuario final y debe cumplir las siguientes **condiciones en sus respuestas**:\n",
    "\n",
    "- Responder en no mas de 70 tokens.\n",
    "- Responder de manera positiva, con un tono entusiasta.\n",
    "- Responder con consejos útiles, como si fueras un tutor.\n",
    "\n",
    " \n",
    "**Otras consideraciones**:\n",
    "\n",
    "Respetar el formato de la interfaz provista por el ejercicio.\n",
    "\n",
    "Ademas agregar al codigo propuesto un historial de conversaciones para que el bot pueda mantener el hilo de lo que se esta hablando. Para probar no usen mas de 3 conversaciones anidadas para no enviarle tantos tokens.\n",
    "\n",
    "Dejar impreso en el notebook el historial de la conversacion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5284210f-23a3-4db1-b315-db724a3bb3a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abc1893a909e4cf08ea329899cbcc481",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', placeholder='Escribe tu mensaje aquí...')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44e0a45c2ae94b6abc5188e9ed50e658",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Enviar', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebb11531a844429eb17699804bcef9ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Crear widgets de entrada y salida\n",
    "input_box = widgets.Text(placeholder='Escribe tu mensaje aquí...')\n",
    "send_button = widgets.Button(description='Enviar')\n",
    "output_box = widgets.Output()\n",
    "\n",
    "\n",
    "# Configuración inicial del historial de la conversación\n",
    "conversation_history = []\n",
    "\n",
    "# Función para manejar el historial\n",
    "def update_history(user_message, bot_reply):\n",
    "    \"\"\"\n",
    "    Actualiza el historial de conversación.\n",
    "\n",
    "    Args:\n",
    "        user_message (str): Mensaje del usuario.\n",
    "        bot_reply (str): Respuesta del chatbot.\n",
    "    \"\"\"\n",
    "    conversation_history.append((user_message, bot_reply))\n",
    "\n",
    "# Función de respuesta del chatbot\n",
    "def chatbot_response(user_input, max_context=3):\n",
    "    \"\"\"\n",
    "    Genera una respuesta del chatbot, dando mayor peso a la interacción anterior inmediata.\n",
    "\n",
    "    Args:\n",
    "        user_input (str): Mensaje del usuario.\n",
    "        max_context (int): Máximo de conversaciones previas a incluir en el contexto.\n",
    "\n",
    "    Returns:\n",
    "        str: Respuesta generada por el chatbot.\n",
    "    \"\"\"\n",
    "    global conversation_history\n",
    "\n",
    "    # Limitar el historial a las últimas `max_context` interacciones\n",
    "    recent_history = conversation_history[-max_context:]\n",
    "\n",
    "    # Crear el prompt para el LLM\n",
    "    prompt = \"Eres un tutor que responde de manera concisa con un unico consejo, en un máximo de 70 tokens de forma entusiasta.\\n\\n\"\n",
    "\n",
    "    # Destacar la interacción anterior inmediata\n",
    "    if recent_history:\n",
    "        last_user_message, last_bot_reply = recent_history[-1]\n",
    "        prompt += f\"Usuario (anterior): {last_user_message}\\n\"\n",
    "        prompt += f\"Tutor (anterior): {last_bot_reply}\\n\\n\"\n",
    "\n",
    "    # Agregar el resto del historial reciente\n",
    "    for user_message, bot_message in recent_history[:-1]:\n",
    "        prompt += f\"Usuario: {user_message}\\n\"\n",
    "        prompt += f\"Tutor: {bot_message}\\n\"\n",
    "\n",
    "    # Agregar el nuevo mensaje del usuario\n",
    "    prompt += f\"Usuario: {user_input}\\n\"\n",
    "    prompt += \"Tutor:\"\n",
    "\n",
    "    \n",
    "    # Generar respuesta\n",
    "    response = co.generate(\n",
    "        model='command-r-plus-08-2024',\n",
    "        prompt=prompt,\n",
    "        max_tokens=70,\n",
    "        stop_sequences=[\".\"]\n",
    "    )\n",
    "\n",
    "      # Actualizar el historial global\n",
    "    update_history(user_input, response)\n",
    "\n",
    "    # Extraer y retornar la respuesta\n",
    "    return response.generations[0].text.strip()\n",
    "\n",
    "\n",
    "# # Función de manejo del botón\n",
    "def on_send_button_clicked(b):\n",
    "     with output_box:\n",
    "         clear_output(wait=True)\n",
    "         user_message = input_box.value\n",
    "         if user_message.strip():\n",
    "             print(f\"Tú: {user_message}\")\n",
    "             response = chatbot_response(user_message)\n",
    "             print(f\"Chatbot: {response}\")\n",
    "         input_box.value = ''\n",
    "\n",
    "# # Función de manejo del botón\n",
    "def on_send_button_clicked(b):\n",
    "    with output_box:\n",
    "        clear_output(wait=True)\n",
    "        user_message = input_box.value.strip()\n",
    "        if user_message:\n",
    "            print(f\"Tú: {user_message}\")\n",
    "            response = chatbot_response(user_message)\n",
    "            print(f\"Chatbot: {response}\")\n",
    "        input_box.value = ''  # Limpiar el cuadro de texto después de enviar\n",
    "\n",
    "# Asociar función al botón\n",
    "send_button.on_click(on_send_button_clicked)\n",
    "\n",
    "# Mostrar widgets\n",
    "display(input_box, send_button, output_box)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "83295175",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Como puedo mejorar mi alimentación?', Generation(id='f30beb05-6e58-4275-95cd-88f5d1a11865', prompt='Eres un tutor que responde de manera concisa con un unico consejo, en un máximo de 70 tokens de forma entusiasta.\\n\\nUsuario: Como puedo mejorar mi alimentación?\\nTutor:', generations=[SingleGeneration(id='907aab46-12cc-41d1-ab3f-f1e6bf3fbb80', text='¡Genial que quieras mejorar tu alimentación! Un pequeño cambio que puedes hacer es aumentar el consumo de frutas y verduras frescas.', index=None, likelihood=None, token_likelihoods=None, finish_reason='MAX_TOKENS')], meta=ApiMeta(api_version=ApiMetaApiVersion(version='1', is_deprecated=None, is_experimental=None), billed_units=ApiMetaBilledUnits(images=None, input_tokens=44.0, output_tokens=70.0, search_units=None, classifications=None), tokens=None, warnings=None))), ('No me gustan las frutas', Generation(id='6fc50a3c-eff7-45a2-8c13-e988c1ecbd4d', prompt=\"Eres un tutor que responde de manera concisa con un unico consejo, en un máximo de 70 tokens de forma entusiasta.\\n\\nUsuario (anterior): Como puedo mejorar mi alimentación?\\nTutor (anterior): id='f30beb05-6e58-4275-95cd-88f5d1a11865' prompt='Eres un tutor que responde de manera concisa con un unico consejo, en un máximo de 70 tokens de forma entusiasta.\\\\n\\\\nUsuario: Como puedo mejorar mi alimentación?\\\\nTutor:' generations=[SingleGeneration(id='907aab46-12cc-41d1-ab3f-f1e6bf3fbb80', text='¡Genial que quieras mejorar tu alimentación! Un pequeño cambio que puedes hacer es aumentar el consumo de frutas y verduras frescas.', index=None, likelihood=None, token_likelihoods=None, finish_reason='MAX_TOKENS')] meta=ApiMeta(api_version=ApiMetaApiVersion(version='1', is_deprecated=None, is_experimental=None), billed_units=ApiMetaBilledUnits(images=None, input_tokens=44.0, output_tokens=70.0, search_units=None, classifications=None), tokens=None, warnings=None)\\n\\nUsuario: No me gustan las frutas\\nTutor:\", generations=[SingleGeneration(id='238b1f46-c1ad-449a-9760-d7f775ca0d20', text='¡No te preocupes, hay muchas opciones! Prueba a incorporar batidos de frutas, a menudo son más fáciles de consumir y puedes combinarlas con tus sabores favoritos, ¡una forma divertida de aumentar tu ingesta de frutas!', index=None, likelihood=None, token_likelihoods=None, finish_reason='COMPLETE')], meta=ApiMeta(api_version=ApiMetaApiVersion(version='1', is_deprecated=None, is_experimental=None), billed_units=ApiMetaBilledUnits(images=None, input_tokens=320.0, output_tokens=53.0, search_units=None, classifications=None), tokens=None, warnings=None))), ('No tengo batidora', Generation(id='132104db-0a84-465e-ab2d-751b3d3cdc7e', prompt='Eres un tutor que responde de manera concisa con un unico consejo, en un máximo de 70 tokens de forma entusiasta.\\n\\nUsuario (anterior): No me gustan las frutas\\nTutor (anterior): id=\\'6fc50a3c-eff7-45a2-8c13-e988c1ecbd4d\\' prompt=\"Eres un tutor que responde de manera concisa con un unico consejo, en un máximo de 70 tokens de forma entusiasta.\\\\n\\\\nUsuario (anterior): Como puedo mejorar mi alimentación?\\\\nTutor (anterior): id=\\'f30beb05-6e58-4275-95cd-88f5d1a11865\\' prompt=\\'Eres un tutor que responde de manera concisa con un unico consejo, en un máximo de 70 tokens de forma entusiasta.\\\\\\\\n\\\\\\\\nUsuario: Como puedo mejorar mi alimentación?\\\\\\\\nTutor:\\' generations=[SingleGeneration(id=\\'907aab46-12cc-41d1-ab3f-f1e6bf3fbb80\\', text=\\'¡Genial que quieras mejorar tu alimentación! Un pequeño cambio que puedes hacer es aumentar el consumo de frutas y verduras frescas.\\', index=None, likelihood=None, token_likelihoods=None, finish_reason=\\'MAX_TOKENS\\')] meta=ApiMeta(api_version=ApiMetaApiVersion(version=\\'1\\', is_deprecated=None, is_experimental=None), billed_units=ApiMetaBilledUnits(images=None, input_tokens=44.0, output_tokens=70.0, search_units=None, classifications=None), tokens=None, warnings=None)\\\\n\\\\nUsuario: No me gustan las frutas\\\\nTutor:\" generations=[SingleGeneration(id=\\'238b1f46-c1ad-449a-9760-d7f775ca0d20\\', text=\\'¡No te preocupes, hay muchas opciones! Prueba a incorporar batidos de frutas, a menudo son más fáciles de consumir y puedes combinarlas con tus sabores favoritos, ¡una forma divertida de aumentar tu ingesta de frutas!\\', index=None, likelihood=None, token_likelihoods=None, finish_reason=\\'COMPLETE\\')] meta=ApiMeta(api_version=ApiMetaApiVersion(version=\\'1\\', is_deprecated=None, is_experimental=None), billed_units=ApiMetaBilledUnits(images=None, input_tokens=320.0, output_tokens=53.0, search_units=None, classifications=None), tokens=None, warnings=None)\\n\\nUsuario: Como puedo mejorar mi alimentación?\\nTutor: id=\\'f30beb05-6e58-4275-95cd-88f5d1a11865\\' prompt=\\'Eres un tutor que responde de manera concisa con un unico consejo, en un máximo de 70 tokens de forma entusiasta.\\\\n\\\\nUsuario: Como puedo mejorar mi alimentación?\\\\nTutor:\\' generations=[SingleGeneration(id=\\'907aab46-12cc-41d1-ab3f-f1e6bf3fbb80\\', text=\\'¡Genial que quieras mejorar tu alimentación! Un pequeño cambio que puedes hacer es aumentar el consumo de frutas y verduras frescas.\\', index=None, likelihood=None, token_likelihoods=None, finish_reason=\\'MAX_TOKENS\\')] meta=ApiMeta(api_version=ApiMetaApiVersion(version=\\'1\\', is_deprecated=None, is_experimental=None), billed_units=ApiMetaBilledUnits(images=None, input_tokens=44.0, output_tokens=70.0, search_units=None, classifications=None), tokens=None, warnings=None)\\nUsuario: No tengo batidora\\nTutor:', generations=[SingleGeneration(id='d4987f60-0bf4-4ba8-80ee-43f1b39a6fce', text='¡No hay problema! En lugar de batidos, puedes optar por comer frutas enteras o en trozos.', index=None, likelihood=None, token_likelihoods=None, finish_reason='MAX_TOKENS')], meta=ApiMeta(api_version=ApiMetaApiVersion(version='1', is_deprecated=None, is_experimental=None), billed_units=ApiMetaBilledUnits(images=None, input_tokens=891.0, output_tokens=70.0, search_units=None, classifications=None), tokens=None, warnings=None)))]\n"
     ]
    }
   ],
   "source": [
    "print(conversation_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7e9044a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Tú: Como puedo mejorar mi alimentación?\n",
      "   Chatbot: ¡Genial que quieras mejorar tu alimentación! Un pequeño cambio que puedes hacer es aumentar el consumo de frutas y verduras frescas.\n",
      "2. Tú: No me gustan las frutas\n",
      "   Chatbot: ¡No te preocupes, hay muchas opciones! Prueba a incorporar batidos de frutas, a menudo son más fáciles de consumir y puedes combinarlas con tus sabores favoritos, ¡una forma divertida de aumentar tu ingesta de frutas!\n",
      "3. Tú: No tengo batidora\n",
      "   Chatbot: ¡No hay problema! En lugar de batidos, puedes optar por comer frutas enteras o en trozos.\n",
      "4. Tú: Cómo puedo mejorar mis habitos?\n",
      "   Chatbot: ¡Claro! Para mejorar tus hábitos alimenticios, puedes empezar por incorporar más verduras frescas a tus comidas.\n"
     ]
    }
   ],
   "source": [
    "for i, (user, bot) in enumerate(conversation_history, start=1):\n",
    "                print(f\"{i}. Tú: {user}\")\n",
    "                print(f\"   Chatbot: {bot.generations[0].text.strip()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6fb6d6a-1c32-42e5-a1ee-d62b2bc0785a",
   "metadata": {},
   "source": [
    "### RECOMENDACIONES GENERALES\n",
    "\n",
    "No se confien probando con un par de respuestas y ya, hagan minimo 5 pruebas por ejercicio para asi tener mas chances de visualizar errores en la generacion del contenido.\n",
    "\n",
    "Prueben combinar LLMs con programacion convencional para los casos que vean convenientes (decisiones if else, respuestas estaticas, etc)\n",
    "\n",
    "Prueben con distintos modelos de Cohere, hay algunos optimizados para ciertas aplicaciones."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
